{"cells":[{"cell_type":"code","source":["# Modulos a utilizar\n","import os\n","import pandas as pd\n","import numpy as np\n","# from google.colab import drive\n","import pyarrow as pa\n","import pyarrow.parquet as pq\n","import pickle\n","\n","import warnings\n","warnings.filterwarnings(action= 'ignore')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"waiting","livy_statement_state":null,"queued_time":"2024-04-09T01:39:58.0114401Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"ba048a74-3b6b-40dc-b968-f12910d4c930"},"text/plain":"StatementMeta(, , , Waiting, )"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"726e7c39-5e4f-4954-84ca-4253546a1b30"},{"cell_type":"code","source":["df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/Copy_Google/metadata_sitios/df_unido.csv\")\n","# df now is a Spark DataFrame containing CSV data from \"Files/Copy_Google/metadata_sitios/df_unido.csv\".\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"a4f3bf4c-a4ec-4739-81ab-4e4ff5b10a20"},{"cell_type":"code","source":["current_directory = os.getcwd()\n","\n","# Mostrar el directorio actual\n","print(\"Estás en el directorio:\", current_directory)\n","desired_directory = '/lakehouse/default/Files\n","'\n","os.chdir(desired_directory)\n","print(\"Ahora Estás en el directorio:\", desired_directory)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"0e43904c-45b3-480f-a769-b448af6780b1","statement_id":6,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-07T04:09:59.9913818Z","session_start_time":null,"execution_start_time":"2024-04-07T04:10:00.5331291Z","execution_finish_time":"2024-04-07T04:10:00.8091368Z","parent_msg_id":"db36c5bb-d864-488f-8deb-2f5f5fb6e2b6"},"text/plain":"StatementMeta(, 0e43904c-45b3-480f-a769-b448af6780b1, 6, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Estás en el directorio: /lakehouse/default/Files/Copy_Google\nAhora Estás en el directorio: /lakehouse/default/Files\n"]}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"a6c0b171-b494-492f-b3d5-394fed639b89"},{"cell_type":"code","source":["def verificar_tipo_datos(df):\n","    '''\n","    Realiza un análisis de los tipos de datos y la presencia de valores nulos en un DataFrame.\n","\n","    Esta función toma un DataFrame como entrada y devuelve un resumen que incluye información sobre\n","    los tipos de datos en cada columna, el porcentaje de valores no nulos y nulos, así como la\n","    cantidad de valores nulos por columna.\n","\n","    Parameters:\n","        df (pandas.DataFrame): El DataFrame que se va a analizar.\n","\n","    Returns:\n","        pandas.DataFrame: Un DataFrame que contiene el resumen de cada columna, incluyendo:\n","        - 'nombre_campo': Nombre de cada columna.\n","        - 'tipo_datos': Tipos de datos únicos presentes en cada columna.\n","        - 'no_nulos_%': Porcentaje de valores no nulos en cada columna.\n","        - 'nulos_%': Porcentaje de valores nulos en cada columna.\n","        - 'nulos': Cantidad de valores nulos en cada columna.\n","    '''\n","\n","    mi_dict = {\"nombre_campo\": [], \"tipo_datos\": [], \"no_nulos_%\": [], \"nulos_%\": [], \"nulos\": []}\n","\n","    for columna in df.columns:\n","        porcentaje_no_nulos = (df[columna].count() / len(df)) * 100\n","        mi_dict[\"nombre_campo\"].append(columna)\n","        mi_dict[\"tipo_datos\"].append(df[columna].apply(type).unique())\n","        mi_dict[\"no_nulos_%\"].append(round(porcentaje_no_nulos, 2))\n","        mi_dict[\"nulos_%\"].append(round(100-porcentaje_no_nulos, 2))\n","        mi_dict[\"nulos\"].append(df[columna].isnull().sum())\n","\n","    df_info = pd.DataFrame(mi_dict)\n","\n","    return df_info"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"editable":true,"run_control":{"frozen":false}},"id":"2facd8d1-4cfb-4584-9e84-acd806b02660"},{"cell_type":"code","source":["def informe_dataframe(dataframe: pd.DataFrame) -> None:\n","\n","    \"\"\"\n","    esta funcion obtiene un dataframe, y realiza un informe analizando y explorando algunas caracteristicas del\n","    dataframe centrandose principalmente en caracteristicas a nivel general de nuestro dataframe y realizando un procesamiento de\n","    algunos datos obteniendo metricas e informacion\n","\n","    devuelve un informe que contiene:\n","\n","    -Dimensiones del DataFrame\n","    -Numero de datos\n","    -Filas y Columnas\n","    -Metricas Generales\n","\n","    Parameters: data (pandas.DataFrame).\n","\n","    Returns: None.\n","\n","    \"\"\"\n","\n","    df = dataframe\n","\n","    print('INFORME PRELIMINAR SOBRE CARACTERISTICAS DEL DATASET:\\n')\n","    print(f'--Dimensiones del DataFrame--\\nFilas: {df.shape[0]}\\nColumnas: {df.shape[1]}\\n')\n","    print(f'--Numero de datos--\\n{df[df.isna() == False].count().sum()}\\n')\n","    print(f'--Filas y Columnas--\\nFilas: muestra de indices-------> {list(df.index)[0:5]}  -----> Desde {list(df.index)[0]}  Hasta {list(df.index)[-1]}\\nColumnas: {list(df.columns)}\\n')\n","    # print(f'--Estadisticos preliminares generales--\\n{df.describe()}\\n')\n","\n","    return ('~'*50)+'oo'+('~'*50)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"b7bca1e7-ebd1-4aa6-8349-fa94ace3e0d7"},{"cell_type":"code","source":["def analisis_estado(estado: str):\n","\n","  \"\"\"\n","  Esta funcion obtiene el nombre de un estado, y llama al archivo .parquet referente al mismo estado obtenido\n","  y lo transforma en un pandas.DataFrame.\n","\n","  Luego realiza un analisis obteniendo diversa informacion del dataframe en general y de los datos presentes\n","  en las columnas en particular imprimiendo sus resultados en pantalla, utilizando las funciones:\n","\n","  - informe_dataframe(df)\n","  - verificar_tip_datos(df)\n","\n","  Parameters: estado (str)\n","  Returns: None\n","  \"\"\"\n","\n","  #url= f\"/content/drive/MyDrive/Proyecto final henry/reviews-estados/review-{estado}/\"\n","  #cantidad_data = len(os.listdir(url))\n","\n","  url = \"/content/drive/MyDrive/Proyecto final henry/merge_estados/\"\n","\n","\n","  df= pd.read_parquet(url + f\"df_unido_{estado}.parquet\")\n","  print(f'Nombre del archivo: {estado}.parquet')\n","  print(f'Estado: {estado}\\n')\n","  print(informe_dataframe(df), end= '\\n\\n')\n","  print(verificar_tipo_datos(df))\n","  print('-'*100)\n","  print('-'*100)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5092a857-9569-4eb2-bd6b-c6e71a823b70"},{"cell_type":"code","source":["def merge_reviews():\n","    ruta_principal = \"/lakehouse/default/Files/Google/reviews-estados\"\n","\n","    # Lista para almacenar los DataFrames de cada archivo JSON\n","    dfs = []\n","\n","    # Recorrer las subcarpetas en la carpeta principal\n","    for subdir, _, archivos in os.walk(ruta_principal):\n","        for archivo in archivos:\n","            if archivo.endswith(\".json\"):\n","                ruta_archivo = os.path.join(subdir, archivo)\n","                # Leer el archivo JSON en un DataFrame de Pandas\n","                df = pd.read_json(ruta_archivo, orient='records', lines=True)\n","                dfs.append(df)\n","\n","    # Concatenar todos los DataFrames en uno solo\n","    df_unido = pd.concat(dfs, ignore_index=True)\n","\n","    # Guardar el DataFrame unificado como un archivo Parquet\n","    output_parquet = \"/lakehouse/default/Files/Copy_Google/Reviews_estados/unificado.parquet\"\n","    df_unido.to_parquet(output_parquet)\n","\n","    print(\"Archivos JSON unificados y convertidos a formato Parquet exitosamente.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"waiting","livy_statement_state":null,"queued_time":"2024-04-09T01:48:45.5668061Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"c328de5f-7cc0-4b75-a774-a803de0577bf"},"text/plain":"StatementMeta(, , , Waiting, )"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Archivos JSON unificados y convertidos a formato Parquet exitosamente.\n"]}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"19b5be49-dc5f-4af3-804d-7cbbc6ab8ce6"},{"cell_type":"code","source":["def merge_dfs_final_metadata():\n","\n","    \"\"\"\n","    Esta funcion concatena los archivos .parquet referentes a metadatos de la carpeta merge_metadata y los guarda en un unico archivo .parquet\n","\n","    Parameters: None\n","    Returns: None\n","    \"\"\"\n","    url = \"/lakehouse/default/Files/Google/metadata-sitios/\"\n","    archivos = os.listdir(url)\n","\n","    # Lista para almacenar los DataFrames de cada archivo JSON\n","    dfs = []\n","\n","    for archivo in archivos:\n","        if archivo.endswith(\".json\"):\n","            ruta_archivo = os.path.join(url, archivo)\n","            df = pd.read_json(ruta_archivo, orient='records', lines=True)\n","            dfs.append(df)\n","    # Concatenar todos los DataFrames en uno solo\n","    df_unido = pd.concat(dfs, ignore_index=True)\n","    # Guardar el DataFrame unificado como un archivo .csv con codificación UTF-8\n","    output_csv = \"/lakehouse/default/Files/Copy_Google/metadata_sitios/df_unido.csv\"\n","    df_unido.to_csv(output_csv, encoding='utf-8', index=False)  # Especificamos la codificación y evitamos guardar el índice\n","    print(df_unido.columns)\n","    print(\"Archivos JSON unificados y convertidos a formato CSV exitosamente.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"47880ad1-74d0-4872-ba3c-0a3fb2a238d0","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-07T18:19:09.1882259Z","session_start_time":null,"execution_start_time":"2024-04-07T18:19:09.6297926Z","execution_finish_time":"2024-04-07T18:22:25.4837274Z","parent_msg_id":"03a39e5e-ea3e-4227-8ff5-8e08a5395df3"},"text/plain":"StatementMeta(, 47880ad1-74d0-4872-ba3c-0a3fb2a238d0, 7, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Index(['name', 'address', 'gmap_id', 'description', 'latitude', 'longitude',\n       'category', 'avg_rating', 'num_of_reviews', 'price', 'hours', 'MISC',\n       'state', 'relative_results', 'url'],\n      dtype='object')\nArchivos JSON unificados y convertidos a formato CSV exitosamente.\n"]}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"dbb510eb-0b6a-431a-b21c-4aa05c48ec6a"},{"cell_type":"markdown","source":["## Nuestros datos son extraidos de dos fuentes principales:"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"481e1833-3662-4643-bd41-282090a81c2d"},{"cell_type":"markdown","source":["# GOOGLE MAPS"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"993c8423-8709-4c85-81d8-bb661c4cd12c"},{"cell_type":"markdown","source":["## metadata-sitios"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e62fdb6e-6e97-43b9-a58e-02efc1ea4cdc"},{"cell_type":"markdown","source":["Uniremos los archivos en 3 archivos parquet que abarquen cada uno 4 archivos y uno 3\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"627594c6-e664-4a08-aee2-7c166aa0a080"},{"cell_type":"code","source":["merge_dfs_final_metadata()\n","merge_reviews()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3e0d064a-1d72-4f78-98ba-c9dac952f363","statement_id":25,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-07T06:44:50.1740326Z","session_start_time":null,"execution_start_time":"2024-04-07T06:44:50.6645969Z","execution_finish_time":"2024-04-07T06:47:09.216416Z","parent_msg_id":"e25075e8-2f5c-4419-995f-d4d403d7d6f2"},"text/plain":"StatementMeta(, 3e0d064a-1d72-4f78-98ba-c9dac952f363, 25, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["               name                                            address  \\\n0   Porter Pharmacy  Porter Pharmacy, 129 N Second St, Cochran, GA ...   \n1      City Textile  City Textile, 3001 E Pico Blvd, Los Angeles, C...   \n2      San Soo Dang  San Soo Dang, 761 S Vermont Ave, Los Angeles, ...   \n3      Nova Fabrics  Nova Fabrics, 2200 E 11th St, Los Angeles, CA ...   \n4  Nobel Textile Co  Nobel Textile Co, 719 E 9th St, Los Angeles, C...   \n\n                                 gmap_id description   latitude   longitude  \\\n0  0x88f16e41928ff687:0x883dad4fd048e8f8        None  32.388300  -83.357100   \n1  0x80c2c98c0e3c16fd:0x29ec8a728764fdf9        None  34.018891 -118.215290   \n2  0x80c2c778e3b73d33:0xbdc58662a4a97d49        None  34.058092 -118.292130   \n3   0x80c2c89923b27a41:0x32041559418d447        None  34.023669 -118.232930   \n4  0x80c2c632f933b073:0xc31785961fe826a6        None  34.036694 -118.249421   \n\n              category  avg_rating  num_of_reviews price  \\\n0           [Pharmacy]         4.9              16  None   \n1   [Textile exporter]         4.5               6  None   \n2  [Korean restaurant]         4.4              18  None   \n3       [Fabric store]         3.3               6  None   \n4       [Fabric store]         4.3               7  None   \n\n                                               hours  \\\n0  [[Friday, 8AM–6PM], [Saturday, 8AM–12PM], [Sun...   \n1                                               None   \n2  [[Thursday, 6:30AM–6PM], [Friday, 6:30AM–6PM],...   \n3  [[Thursday, 9AM–5PM], [Friday, 9AM–5PM], [Satu...   \n4  [[Thursday, 9AM–5PM], [Friday, 9AM–5PM], [Satu...   \n\n                                                MISC              state  \\\n0  {'Service options': ['In-store shopping', 'Sam...  Open ⋅ Closes 6PM   \n1                                               None           Open now   \n2  {'Service options': ['Takeout', 'Dine-in', 'De...  Open ⋅ Closes 6PM   \n3  {'Service options': ['In-store shopping'], 'Pa...  Open ⋅ Closes 5PM   \n4           {'Service options': ['In-store pickup']}  Open ⋅ Closes 5PM   \n\n                                    relative_results  \\\n0  [0x88f16e41929435cf:0x5b2532a2885e9ef6, 0x88f1...   \n1  [0x80c2c624136ea88b:0xb0315367ed448771, 0x80c2...   \n2  [0x80c2c78249aba68f:0x35bf16ce61be751d, 0x80c2...   \n3  [0x80c2c8811477253f:0x23a8a492df1918f7, 0x80c2...   \n4  [0x80c2c62c496083d1:0xdefa11317fe870a1, 0x80c2...   \n\n                                                 url  \n0  https://www.google.com/maps/place//data=!4m2!3...  \n1  https://www.google.com/maps/place//data=!4m2!3...  \n2  https://www.google.com/maps/place//data=!4m2!3...  \n3  https://www.google.com/maps/place//data=!4m2!3...  \n4  https://www.google.com/maps/place//data=!4m2!3...  \nArchivos JSON unificados y convertidos a formato Parquet exitosamente.\n"]}],"execution_count":23,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"454dd056-4429-4968-8e61-ff2bc426464a"},{"cell_type":"markdown","source":["# YELP"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5232c963-f985-41a8-b0dd-d84c56c34879"},{"cell_type":"markdown","source":["### Bussines"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"56e080eb-27f2-4c33-92e0-bc91332b6792"},{"cell_type":"code","source":["#url de fichero bussines.pkl\n","bussi_yelp= \"/content/drive/MyDrive/Proyecto final henry/Yelp/business.pkl\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"df53f82d-2e74-4386-81fe-83168cb989d5"},{"cell_type":"code","source":["# Abrir el archivo en modo de lectura binaria\n","with open(bussi_yelp, 'rb') as f:\n","    # Cargar el objeto desde el archivo .pkl\n","    objeto = pickle.load(f)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e45085d6-ce1e-40a3-bb58-7438403ec554"},{"cell_type":"code","source":["# transformamos la data en un dataframe\n","df_bussi = pd.DataFrame(objeto)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4b49a123-d904-481a-a927-9ddafa56abb9"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"es"}},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"680b0e14-dff3-4df6-b2f1-998aa9bdaa2d","default_lakehouse_name":"Datalake_Origen","default_lakehouse_workspace_id":"558991ad-6fdf-463a-b58a-238ddabaa398"}}},"nbformat":4,"nbformat_minor":5}